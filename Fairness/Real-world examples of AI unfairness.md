There are numerous examples of AI applications that exhibited unwanted discriminatory behaviors. Alarming for the need to take action to overcome the potential bias that might be embedded in AI systems.   

#### The COMPAS software
One of the most prominent examples is the **Correctional Offender Management Profiling for Alternative Sanctions** (Compas) system. A 
software used in the US to assess the recidivism risk of defendants. 
Julia Angwin [investigated](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) the software and showed that compared to white defendants, black defendants are predicted as twice as likely to re-offend although they do no subsequent offenses. While white defendants with subsequent offenses are predicted as low risk compared to their black counterparts. In order words, the software had a higher false positive rate for black defendants and a higher false negative rate for white defendants.  

### Amazone gender bias hiring system
Another example is the AI-based [hiring system](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G) used by Amazon for assessing the resumes of job applicants. It was observed that the evaluations assigned to applicants' resumes exhibited gender bias. Women received lower scores for technical positions compared to their male compared. In fact, due to the underrepresentation of women in technical job positions, the software inherently linked male applicants to higher scores.   Although gender information was not explicitly used in the resumes, the system was able to infer it via women's related information in the resume such as *women’s chess club captain* or attendance to all women’s colleges. 

### Google Translate gender bias 
Gender bias in machine translation was [shown](https://blog.google/products/translate/reducing-gender-bias-google-translate/) when translating text between gender-neutral languages such as Turkish and other languages such as English and French. In the process of translating a sentence, it is observed that the system often assigns male pronouns (he, him) to terms like Doctor and Software, while female pronouns are predominantly assigned to words like Nurse or Hometaker.

### Twitter chatbot  Tay
Microsoft launched a [Twitter chatbot](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist) named *Tay* that can answer tweets from users. The bot was trained on tweets crawled from the web and the goal was to experiment with the potential conversational AI. Across initial interactions, the chatbot started throwing racist, antisemite, or sexist content. The tool was turned off after a few hours and this drama served as a good example of the importance of avoiding training AI models with uncurated content from the web.

### Bias in the facial recognition system. 
Face recognition algorithms claim to achieve high classification accuracy, surpassing 90%. However, these results do not apply uniformly. A growing body of research reveals [significant variations](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/) in error rates among different demographic groups, particularly in the accuracy of female individuals, Black individuals, and those aged 18-30. In a significant project called "Gender Shades" conducted in 2018, researchers employed an intersectional approach to evaluate gender classification algorithms, including those developed by IBM and Microsoft. Participants were categorized into four groups: darker-skinned females, darker-skinned males, lighter-skinned females, and lighter-skinned males. Remarkably, all three algorithms performed the poorest in identifying darker-skinned females, with error rates up to 34% higher compared to lighter-skinned males. These findings have been further supported by the National Institute of Standards and Technology (NIST), which independently assessed 189 face recognition algorithms and confirmed that the technologies are least accurate when identifying women of color.

