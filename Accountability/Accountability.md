We have seen in previous sections that AI systems can sometimes go wrong, for example, the Twitter bot throwing hateful and racist content, gender bias in the Amazon hiring system, or adversarial environments that mislead a deep neural network model. In general, when something goes wrong in an organization or a company, someone is held accountable for it. However, the AI system itself cannot take responsible for unpredicted outcomes. A question that naturally arises when AI systems provide outcomes that have adverse effects or harm individuals, is who is accountable for the issues. Accountability is a well-studied notion in different fields (politic, law). It provides moral principles that guide the ethical conduct of people or organizations as they bear the responsibility for their actions. Accountability is, therefore, a key component of trust in society, organizations, and the professional milieu. In the context of AI, accountability is a meta-component of trustworthiness that ensure that ethical principles are promoted and enforced throughout the lifecycle of AI projects. 

## Accountable for what?

[According](https://link.springer.com/book/10.1007/978-3-030-30371-6?trk=public_post_comment-text) to Virginia Dignum accountability means the decisions of the AI system are explainable and derivable from the decision-making mechanisms used and is a set of components guided by moral values that are part of a large socio-technical system. [Millar, Jason, et al](https://cifar.ca/wp-content/uploads/2020/11/g7-accountabilityinai.pdf) see accountability as "answerability" for decisions, actions, products, and policies. From this perspective, it is required that accountability is a practice that the management board of a company could enforce by making all the stakeholders who develop the system understand that they bear the responsibility for the decisions of the system. Therefore, policymakers, data scientists, and AI engineers should carefully choose and justify the choices made during the design, development, and deployment of AI systems ([Novelli, Claudio, et al.](https://link.springer.com/article/10.1007/s00146-023-01635-y)).         

## Accountable to whom?

The design, development, and deployment of AI systems involve multiple stakeholders. While a system in which decisions can be explained can help to detect the causes or reasons for the incidents, it does not necessarily provide an answer to who is most responsible for the unintended outcomes. Establishing causal accountability remains challenging, and unintended negative outcomes might bear legal liability. Companies or organizations can acknowledge incidents caused by the use of their systems and can pay for reparation. However, monetary compensation could not be enough in society and requires someone to get punished, i.e., to take legal responsibility for the potential harm. [Ammanath, Beena](https://books.google.ca/books?hl=fr&lr=&id=dcV6EAAAQBAJ&oi=fnd&pg=PR11&dq=ammanath+2022+trustworthy&ots=nDcLWuXHXo&sig=H3INWtCnDSh_leGj36tmcI7tbKo&redir_esc=y#v=onepage&q=ammanath%202022%20trustworthy&f=false),  says that "*instead of looking for people to blame, companies should recognize the legal responsibility of sociotechnical systems around AI and all stakeholders should use it as an "additional motivation to own their individual responsibility*".   

Promoting accountability throughout the entire lifecycle of AI systems faces the challenge of developing processes and rules to enforce it while not hindering innovation. The enforcement of accountability can also be baked by laws and regulations that define a conformity assessment of the outcome of sociotechnical systems in order to hinder potential flaws of the system ([Ammanath, Beena](https://books.google.ca/books?hl=fr&lr=&id=dcV6EAAAQBAJ&oi=fnd&pg=PR11&dq=ammanath+2022+trustworthy&ots=nDcLWuXHXo&sig=H3INWtCnDSh_leGj36tmcI7tbKo&redir_esc=y#v=onepage&q=ammanath%202022%20trustworthy&f=false), [Millar, Jason, et al](https://cifar.ca/wp-content/uploads/2020/11/g7-accountabilityinai.pdf)). Accountability, therefore, becomes a core component of trustworthiness in a socio-technical system. That is, if a system provides discriminatory outcomes, leaks people's sensitive data, or is not robust to different kinds of environments where it is deployed, someone would bear the responsibility of justifying and mitigating it.     